{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EmoteDet: Emotion Detection for Artist Discography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Implementation of a **Data Pipeline** to analyze the discography from an **Artist**, using Machine Learning to detect the **emotions** associated to the lyrics of their tracks, and then gathering and visualizing the data to have a general idea of the emotions conveyed by the artist in their music, adding new music in **real time** when released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first problem to solve to implement the idea, is to decide where to gather the data, meaning the lyrics from all the discography from an artist.\n",
    "The choice fell on a crawler written in **Python**, using two services to gather the data: **Spotify and Genius.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"notebook/crawler.png\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why two APIs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The question now is: why using two services when you could use just one?  \n",
    "Well, weirdly, the answer is **efficency**.  \n",
    "  \n",
    "Using only the Genius APIs, which are the ones used to actually get the lyrics into our pipeline, the search times for all the songs were way too long, because the actual crawler first researches the artist name, and then from the object returned, would download every single lyrics one by one searching the track by artist, saving them all only at the end of the last search.  \n",
    "  \n",
    "Now this would be pretty bad for a real-time project like this. So the solution was to actually get all the track titles before searching for the lyrics, but with the Python API Interface for Genius (**lyricsgenius**) this was not possible. Here enters Spotify.  \n",
    "  \n",
    "Using **Spotipy**, the Python API Interface for Spotify, the crawler first searches for all the track titles, which then are read by the Genius APIs which actually get the lyrics track by track.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's look at some code now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's finally time to start looking at the **code!**\n",
    "The crawler is a little Python Script which uses lyricsgenius and spotipy to actually get our data.\n",
    "We will look at all the **functions** wrote to make the pieces work togheter now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "def searchArtist(artistName, spotify):\n",
    "    \n",
    "    result = spotify.search('{' + artistName + '}',type=\"artist\")\n",
    "    return result['artists']['items'][0]['uri']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We start by searching for the Artist into the Spotify database with their APIs. The search returns a **JSON response**, from which we extract the only field we need: the Spotify **URI related to the Artist.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def searchAlbums(artistUri, spotify):Ã¹\n",
    "    album_uris = []\n",
    "    sp_albums = spotify.artist_albums(artistUri, album_type='album')\n",
    "\n",
    "    for i in range(len(sp_albums['items'])):\n",
    "        album_uris.append(sp_albums['items'][i]['uri'])\n",
    "    return album_uris\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using the Artist URI, now we can get all the **Albums** published by the artist we are looking for. The only information we need is the URI again, so we get a list of **Album URIs**, extracted within the APIs JSON response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def searchTracks(albumUris, spotify):\n",
    "    trackList = []\n",
    "    for uri in albumUris:\n",
    "        albumTracks = spotify.album_tracks(uri)\n",
    "        for n in range(len(albumTracks['items'])):\n",
    "            if albumTracks['items'][n]['name'] not in trackList:\n",
    "                trackList.append(albumTracks['items'][n]['name'])\n",
    "    return trackList\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We finally have what we needed: using the URIs from the album list, the function can get all the **track titles** we needed, again extracted from the APIs JSON response, which are then returned as a list of titles, checking for duplicates (we do not want the same track to be analyzed twice). The track titles will be **sanitized** right after (meaning we remove all the characters that will raise Exceptions during the save process or make the research more difficult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def getTrackData(track, genius, artistName):\n",
    "  \n",
    "    trackData = genius.search_song(title=track, artist=artistName)\n",
    "    if trackData != None:\n",
    "        return trackData.to_json()\n",
    "    else:\n",
    "        return None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We are almost at the end: now, having all the track titles, this function will be called for each of them. The result will be a JSON response from Genius, with all the data we need for the track (**title, artist, album, lyrics, ecc)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "def writeTrackData(trackData, track):\n",
    "    filename = \"/opt/tracks/\" + track + \".json\"\n",
    "    file = open(filename, 'w')\n",
    "    file.write(trackData)\n",
    "    file.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, the crawler will write the data on a **JSON file**, ready for the ingestion into our Data Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The journey begins\n",
    "\n",
    "It's time for our data to start traversing our Pipeline, starting with the Data Ingestion:\n",
    "We will be using [**Logstash**](https://www.elastic.co/logstash) as our Data Ingestion layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"notebook/cralog.png\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Logstash will read all the JSON files the crawler will create in real time, **logging** them as soon as the new file will be created. Here's the configuration for the **Logstash Pipeline** (with a little spolier for our next layer!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```json\n",
    "input {\n",
    "    file {\n",
    "        path => [\"/opt/tracks/*.json\"]\n",
    "         codec => \"json\"\n",
    "        mode => \"read\"\n",
    "    }\n",
    "}\n",
    "\n",
    "filter {\n",
    "    json {\n",
    "        source => \"message\"\n",
    "    }\n",
    "}\n",
    "\n",
    "output {\n",
    "    kafka {\n",
    "        topic_id => \"emotedet\"\n",
    "        bootstrap_servers => \"kafkaserver:9092\"\n",
    "        codec => json\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time to deliver our messages\n",
    "\n",
    "The next step in our Pipeline is to actually deliver our data to the recipient (the Data Analysis layer)\n",
    "This will be our **Data Streaming** layer, and the choice for it is [**Apache Kafka**](https://kafka.apache.org/), an open-source distributed event streaming platform. Logstash will write all the data ingested into a Kafka **topic**, ready to be read by the Analysis layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"notebook/cralogkaf.png\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's detect some emotions!\n",
    "\n",
    "We've reached the heart of the Pipeline now: it's time to analyze our data to actually do the **Emotion Detection** on the lyrics we've gotten by Genius.   \n",
    "  \n",
    "Our **Data Analyis** layer will be executed by [**Apache Spark**](https://spark.apache.org/), a unified analytics engine for data processing, in particular using two modules of Spark: **Spark ML**, the Machine Learning library to actually classify our text, and **Spark Structured Streaming**, to read and write data in real-time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"notebook/cralogkafspa.png\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spark ML\n",
    "\n",
    "Apache Spark offers a library called Spark ML, to use Machine Learning functions to actually analyze data based on Spark's own type of container, the **Dataframe**.\n",
    "\n",
    "We built a **ML Pipeline** to train our model first, using the **[ISEAR dataset](https://www.unige.ch/cisa/research/materials-and-online-research/research-material/)**, derived by a study from the University of Geneva where respondents where asked to report situations where they experienced one of the seven major emotions (**joy, fear, anger, sadness, disgust, shame, and guilt**). The final project was to actually build a dataset based on the data collected by the study, which we'll be using for our own project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ML code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "trainingSet = spark.read.csv(\"/opt/spark/dataset/DATA.csv\",\n",
    "                            schema=schema,\n",
    "                            header=True,\n",
    "                            sep=';')\n",
    "\n",
    "\n",
    "trainingSet = trainingSet.na.drop(subset=\"lyrics\")\n",
    "\n",
    "removerToGetList = StopWordsRemover(inputCol=\"words\", outputCol=\"wordsSanitized\")\n",
    "stopWordList = removerToGetList.getStopWords()\n",
    "stopWordList.extend(['verse', 'refrain', 'intro', 'outro', 'bridge', 'chorus', '1', '2', '3', 're','pre', 'oh', 'll', 'solo', 've', 'yeah', 'm'])\n",
    "```\n",
    "\n",
    "We start by reading our dataset into a **Spark Dataframe**, and preparing our first pieces for the ML Pipeline: getting the **Stop Words** to remove from the phrases words and adding music-related ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "stage_1 = RegexTokenizer(inputCol=\"lyrics\", outputCol=\"words\", pattern='\\\\W')\n",
    "\n",
    "stage_2 = StopWordsRemover(inputCol=\"words\", outputCol=\"wordsSanitized\", stopWords=stopWordList)\n",
    "\n",
    "stage_3 = CountVectorizer(inputCol=\"wordsSanitized\", outputCol=\"vector\", minDF=1.0)\n",
    "\n",
    "stage_4 = StringIndexer(inputCol=\"emotion\", outputCol=\"emotionLabel\")\n",
    "\n",
    "model = LogisticRegression(featuresCol=\"vector\", labelCol=\"emotionLabel\")\n",
    "\n",
    "#creating ML Pipeline\n",
    "pipeline = Pipeline(stages=[stage_1, stage_2, stage_3, stage_4, model])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then we actually build our pipeline, using five stages:  \n",
    "\n",
    "1. We use a **RegexTokenizer** to split the phrases word by word;\n",
    "2. Then a **StopWordsRemover** to remove stop words from our analysis;\n",
    "3. Then we pass our words through a **CountVectorizer**, to get the corresponding Frequency Vector to use in our analysis;\n",
    "4. We use a **StringIndexer** to \"cast\" our emotions from strings to int, using indexes.\n",
    "5. Then we finally use **Logistic Regression** to actually do the analysis and get our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "pipelineFit = pipeline.fit(trainingSet)\n",
    "\n",
    "labels = pipelineFit.stages[3].labels\n",
    "\n",
    "evaluationData = pipelineFit.transform(trainingSet)\n",
    "\n",
    "reindexer = IndexToString(inputCol=\"prediction\", outputCol=\"predictedEmotion\", labels=labels)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"emotionLabel\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(evaluationData)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once our pipeline is built, we'll fit the training set to our pipeline, and get the correspondent labels for our emotion to actually get the string and not the index. We then use a **MulticlassClassificationEvaluator** to actually get the **accuracy** for our model: *0.9868159176517879*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "trackDataFrame = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"kafkaserver:9092\") \\\n",
    "                .option(\"subscribe\", \"emotedet\") \\\n",
    "                .option(\"startingOffsets\", \"earliest\").load()\n",
    "\n",
    "trackDataFrame = trackDataFrame.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "                .select(from_json(\"value\", lyricsSchema).alias(\"data\")) \\\n",
    "                .select(\"data.*\")\n",
    "\n",
    "trackDataFrame = reindexer.transform(pipelineFit.transform(trackDataFrame)).select('id', 'title', 'artist', 'lyrics','wordsSanitized', 'header_image_url', 'predictedEmotion')\n",
    "\n",
    "trackDataFrame.withColumnRenamed('wordsSanitized', 'words').withColumn(\"timestamp\", current_timestamp()).writeStream \\\n",
    "            .option(\"checkpointLocation\", \"/save/location\")\\\n",
    "                .format(\"es\")\\\n",
    "                .start(\"emotedet\")\\\n",
    "                .awaitTermination()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our model is ready: we can now **read** our data from Kafka in real time using **Spark Structured Streaming**, and then applying our model to the data read to actually **analyze** it. After the analysis, we'll write the data somewhere (no spoilers!) and we will be ready to **visualize** it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time for pie charts!\n",
    "\n",
    "Now that our data is analyzed, we are ready to visualize our results! We'll use [**Kibana**](https://www.elastic.co/kibana),  a free and open user interface that lets you visualize your data, to  have more **insight** on the artist discography. To use Kibana though, we need Spark to write the data on **[Elasticsearch](https://www.elastic.co/elasticsearch/)**, a distributed, JSON-based search and analytics engine, to organize our data ready for Kibana to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"notebook/almostdone.png\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When the project will be deployed (we'll se later how!), Kibana will be ready with an **Index Pattern** to match the Elasticsearch index created by Spark, and a **Dashboard** to visualize some basic data from the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"notebook/dashboard.png\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When 16 GB of memory is not enough\n",
    "\n",
    "Our data pipeline is **completed and running** now, but how much our devices will be **affected** by it? Will my PC explode or it will survive?   \n",
    "   \n",
    "The deployement of the pipeline will include a system to collect **metrics** from the various layers of our pipeline, and the choice was to use **[Metricbeat](https://www.elastic.co/beats/metricbeat)**, a lightweight way to send system and service **statistics**, based on the ELK stack and Kibana-ready for the visualization of our metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"notebook/done.png\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Metrics\n",
    "\n",
    "With **Metricbeats**, we can get all the metrics from the ELK stack directly on Kibana, all the data from our Docker containers (because the project is intended to be deployed using containers, we'll discuss about it later on), and various metrics from our Kafka broker.\n",
    "   \n",
    "Both the Docker and Kafka module will have their **dashboard** ready to visualize the metrics, while the ones related to the ELK stack can be checked directly on Kibana's **Stack Management** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```yaml\n",
    "metricbeat.config.modules:\n",
    "  path: ${path.config}/modules.d/*.yml\n",
    "  reload.enabled: false\n",
    "\n",
    "processors:\n",
    "  - add_cloud_metadata: ~\n",
    "  - add_docker_metadata: ~\n",
    "\n",
    "output.elasticsearch:\n",
    "  hosts: 'http://elastic:9200'\n",
    "\n",
    "setup.kibana.host: \"kibana:5601\"\n",
    "metricbeat.modules:\n",
    "- module: docker\n",
    "  metricsets:\n",
    "    - \"container\"\n",
    "    - \"cpu\"\n",
    "    - \"diskio\"\n",
    "    - \"event\"\n",
    "    - \"healthcheck\"\n",
    "    - \"info\"\n",
    "    - \"memory\"\n",
    "    - \"network\"\n",
    "  hosts: [\"unix:///var/run/docker.sock\"]\n",
    "  period: 10s\n",
    "  enabled: true\n",
    "\n",
    "# Kafka metrics collected using the Kafka protocol\n",
    "- module: kafka\n",
    "  metricsets:\n",
    "    - partition\n",
    "    - consumergroup\n",
    "  period: 10s\n",
    "  hosts: [\"kafkaserver:9092\"]\n",
    "\n",
    "# Metrics collected from a Kafka broker using Jolokia\n",
    "- module: kafka\n",
    "  metricsets:\n",
    "    - broker\n",
    "  period: 10s\n",
    "  hosts: [\"kafkaserver:8779\"]\n",
    "\n",
    "- module: logstash\n",
    "  period: 10s\n",
    "  hosts: [\"logstash:9600\"]\n",
    "  xpack.enabled: true\n",
    "\n",
    "\n",
    "- module: kibana\n",
    "  period: 10s\n",
    "  hosts: [\"kibana:5601\"]\n",
    "  enabled: true\n",
    "  xpack.enabled: true\n",
    "\n",
    "- module: elasticsearch\n",
    "  period: 10s\n",
    "  hosts: [\"elastic:9200\"]\n",
    "  xpack.enabled: true\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Containers orchestration\n",
    "\n",
    "The project is intended to be deployed using **[Docker containers](https://www.docker.com/resources/what-container)**, a standard unit of software that packages up code and all its dependencies so the application runs **quickly and reliably from one computing environment to another.**  \n",
    "  \n",
    "All the layers of the pipeline are already Docker-ready, and the project can be deplyed using two different container orchestration methods: **Docker-Compose and Kubernetes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Docker-Compose\n",
    "\n",
    "The project includes a docker-compose.yml file, ready to be passed to the **[Docker-Compose CLI](https://docs.docker.com/compose/)** and deployed as a series of Docker containers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```yaml\n",
    "version: \"3\"\n",
    "services:\n",
    "    logstash:\n",
    "        build: logstash\n",
    "        container_name: \"logstash\"\n",
    "        image: \"logstash:emotedet\"\n",
    "        depends_on:\n",
    "            - \"crawler\"\n",
    "            - \"kafkaserver\"\n",
    "        ports:\n",
    "            - \"9600:9600\"\n",
    "        volumes:\n",
    "            - \"tracks:/opt/tracks\"\n",
    "        networks:\n",
    "            - \"emotedet\"\n",
    "\n",
    "    # Crawler container\n",
    "    crawler:\n",
    "        build: \"geniusCrawler\"\n",
    "        image: \"crawler:emotedet\"\n",
    "        container_name: \"crawler\"\n",
    "        depends_on:\n",
    "            - \"kafkaserver\"\n",
    "        volumes: \n",
    "            - \"tracks:/opt/tracks\"\n",
    "        environment:\n",
    "            - \"PYTHONUNBUFFERED=1\"\n",
    "            - \"ARTIST\"\n",
    "    \n",
    "    # Zookeeper for Kafka\n",
    "    zookeeper:\n",
    "        image: \"confluentinc/cp-zookeeper:6.1.1\"\n",
    "        container_name: \"zookeeper\"\n",
    "        ports:\n",
    "            - \"2181:2181\"\n",
    "        environment:\n",
    "            ZOOKEEPER_CLIENT_PORT: \"2181\"\n",
    "            ZOOKEEPER_SERVER_ID: \"1\"\n",
    "        networks:\n",
    "            - \"emotedet\"\n",
    "\n",
    "\n",
    "    # Kafka Broker\n",
    "    kafkaserver:\n",
    "        build: \"kafka\"\n",
    "        image: \"kafka:emotedet\"\n",
    "        container_name: \"kafkaserver\"\n",
    "        hostname: \"kafkaServer\"\n",
    "        depends_on:\n",
    "            - \"zookeeper\"\n",
    "        ports:\n",
    "            - \"9092:9092\"\n",
    "            - \"9101:9101\"\n",
    "            - \"8779:8779\"\n",
    "        environment:\n",
    "            KAFKA_BROKER_ID: 0\n",
    "            KAFKA_ZOOKEEPER_CONNECT: \"zookeeper:2181\"\n",
    "            KAFKA_ADVERTISED_LISTENERS: \"PLAINTEXT://kafkaserver:9092\"\n",
    "            KAFKA_DEFAULT_REPLICATION_FACTOR: 1\n",
    "            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "            KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n",
    "            KAFKA_OPTS: \"-javaagent:/opt/jolokia-jvm-1.6.2-agent.jar=port=8779,host=kafkaserver\"\n",
    "        networks:\n",
    "            - \"emotedet\"\n",
    "\n",
    "\n",
    "    # Container to create Kafka topic\n",
    "    kafkatopic:\n",
    "        image: \"confluentinc/cp-kafka:6.1.1\"\n",
    "        container_name: \"kafkatopic\"\n",
    "        depends_on:\n",
    "            - \"kafkaserver\"\n",
    "        command: bash -c \"kafka-topics --create --bootstrap-server kafkaserver:9092 --replication-factor 1 --partitions 1 --topic emotedet\"\n",
    "        networks:\n",
    "            - \"emotedet\"\n",
    "\n",
    "    # Kafka webui\n",
    "    webui:\n",
    "        image: \"provectuslabs/kafka-ui:latest\"\n",
    "        container_name: \"kafkaWebUI\"\n",
    "        environment:\n",
    "            KAFKA_CLUSTERS_0_NAME: \"my_cluster\"\n",
    "            KAFKA_CLUSTERS_0_ZOOKEEPER: \"zookeeper:2181\"\n",
    "            KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: \"kafkaServer:9092\"\n",
    "        ports:\n",
    "            - \"8080:8080\"\n",
    "        depends_on:\n",
    "            - \"kafkaserver\"\n",
    "        networks:\n",
    "            - \"emotedet\"\n",
    "\n",
    "\n",
    "    # Spark container\n",
    "    spark:\n",
    "        build: \"spark\"\n",
    "        image: \"spark:emotedet\"\n",
    "        ports:\n",
    "            - \"4040:4040\"\n",
    "        container_name: \"spark\"\n",
    "        depends_on:\n",
    "            - \"kafkaserver\"\n",
    "            - \"kibana\"\n",
    "        networks:\n",
    "            - \"emotedet\"\n",
    "\n",
    "    # ES container\n",
    "    elastic:\n",
    "        image: \"docker.elastic.co/elasticsearch/elasticsearch:7.13.2\"\n",
    "        container_name: \"elastic\"\n",
    "        environment:\n",
    "            - \"node.name=es\"\n",
    "            - \"cluster.name=es-docker-cluster\"\n",
    "            - \"discovery.seed_hosts=es\"\n",
    "            - \"discovery.type=single-node\"\n",
    "            - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n",
    "        ports:\n",
    "            - \"9200:9200\"\n",
    "        networks:\n",
    "            - \"emotedet\"\n",
    "\n",
    "    # Kibana container\n",
    "    kibana:\n",
    "        image: \"docker.elastic.co/kibana/kibana:7.12.1\"\n",
    "        container_name: \"kibana\"\n",
    "        ports:\n",
    "            - \"5601:5601\"\n",
    "        depends_on:\n",
    "            - \"elastic\"\n",
    "        networks:\n",
    "            - \"emotedet\"\n",
    "        environment:\n",
    "            - ELASTICSEARCH_HOSTS=\"http://elastic:9200\"\n",
    "\n",
    "\n",
    "    # Metricbeat container\n",
    "    metricbeat:\n",
    "        build: \"metricbeat\"\n",
    "        image: \"metricbeat:emotedet\"\n",
    "        container_name: \"metricbeat\"\n",
    "        restart: \"always\"\n",
    "        environment:\n",
    "            ELASTICSEARCH_HOSTS: \"elastic:9200\"\n",
    "        volumes:\n",
    "            - \"metricbeat:/usr/share/metricbeat/data\"\n",
    "            - \"/var/run/docker.sock:/var/run/docker.sock\"\n",
    "        networks:\n",
    "            - \"emotedet\"\n",
    "\n",
    "    # Container to import Kibana dashboards and indexes via REST Api\n",
    "    kibImporter:\n",
    "        build: \"kibImporter\"\n",
    "        image: \"kibmporter:emotedet\"\n",
    "        container_name: \"kibImporter\"\n",
    "        depends_on:\n",
    "            - \"kibana\"\n",
    "        networks:\n",
    "            - \"emotedet\"\n",
    "\n",
    "volumes:\n",
    "    tracks:\n",
    "    metricbeat:\n",
    "\n",
    "networks:\n",
    "    emotedet:\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Aside from our pipeline layers, the compose files will create a couple of other containers:  \n",
    "\n",
    "* A Container for **[Zookeeper](https://zookeeper.apache.org/)**, needed by Kafka for running\n",
    "* A Container to create the **Kafka Topic** that will be written by Logstash and read by Spark\n",
    "* A Container to import all the **Kibana Dashboards and indexes** needed to visualize the data and the metrics.\n",
    "\n",
    "Having Docker installed, deploying the pipeline will be pretty easy: we will need just a command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "docker-compose up --build -d\n",
    "```\n",
    "\n",
    "Let's see what this does: ```docker-compose up``` will say to docker to run all the containers defined in the docker-compose file with all the options indicated, ```--build``` will actually build all the images needed for the various containers, and ```-d``` will tell the CLI to not log all the containers in the current terminal\n",
    "\n",
    "Once the containers are all up and running, we can enter into **Kibana** using a browser with the url [kibana:5601](http://kibana:5601), and start playing with our data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kubernetes\n",
    "\n",
    "The project offers, other than a Docker-Compose solution, all the files needed to deploy it to a **Kubernetes Cluster**.\n",
    "\n",
    "**[Kubernetes](https://kubernetes.io/)**, also known as **K8s**, is an open-source system for automating deployment, scaling, and management of containerized applications. Using the **kubectl** CLI, assumed that a K8s cluster is already up and running, we can deploy the project easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first thing to do to deploy the pipeline to a Kubernete cluster, is to be sure all the **images** are built and updated. We can easily do that in a single command using the docker-compose file:\n",
    "```bash\n",
    "docker-compose build```\n",
    "\n",
    "Once the images are ready, and a **Kubernetes Cluster** is online (es. using [**Minikube**](https://minikube.sigs.k8s.io/docs/) or the Docker Desktop Kubernetes cluster), we can deploy all the containers into **K8s pods** using kubectl:\n",
    "```bash\n",
    "kubectl apply -f kubernetes\n",
    "```\n",
    "The command will apply to the cluster all the services and deployments descripted in the files in the **kubernetes** folder. If a **Kubernetes Dashboard** is up, we can check that all is running smoothly there (es. Using minikube we can start it with the command ```minikube dashboard ```)\n",
    "\n",
    "To actually access **Kibana**, we need to tell Kubernetes to forward its **port** to our host system. We can do that using kubectl.\n",
    "\n",
    "```bash\n",
    "kubectl port-forward kibana-pod-name 5601```\n",
    "\n",
    "Once all is running, we can finally access our data with Kibana in our browser at the link [localhost:5601](localhost:5601)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
